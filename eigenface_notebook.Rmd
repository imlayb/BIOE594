---
title: "Eigenface Notebook"
params:
  n_features: 1000
bibliography: eigenfaces.bib
output:
  html_document:
    df_print: paged
  html_notebook: default
---
```{r required packages,message=FALSE,warning=FALSE}
source("eigenfaces.R")
library(plyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(gridExtra)
library(factoextra)
library(caret)
```
## Introduction
The 1991 article *Eigenfaces for Recognition* was formative for the multimedia research fields [@Turk1991]. The dataset, which predates the *Eigenfaces for Recognition* article, has since become a benchmark for computer vision and machine learning.


The data consists of 3993 pictures of human faces at a resolution of 128 x 128 in 8-bit grayscale. Data can be obtained from [MIT](http://courses.media.mit.edu/2004fall/mas622j/04.projects/faces/).


## Data Import
The eigenface rawdata is packaged with this repository and can be loaded with the following code. Two pictures were remove from the rawdata as provided from MIT because they were of incorrect dimensions: 2416 and 2412. Moreover, 22 faces are removed for having no parsed information.

```{r data import, message=FALSE, warning=FALSE, include=TRUE}
source("eigenfaces.R")
dat<-importFaceMatrix()
```

Each picture is represented as vector of length 128^2^=16,384. This vector can be reshaped into a raster, which can be colored with a color scale — grayscale in our case. Three random faces have been generated below.

```{r plot images,fig.height=3,fig.width=9,collapse=TRUE,results= 'hold'}
#This code may be run multiple times to see the huge diversity of images in the data.
par(mfrow=c(1,3))
selected<-sample(rownames(dat),3)
{plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])}
```

## Metadata Import

The metadata comes from MIT already split into two sets, training and testing.

```{r meta import,message=FALSE,warning=FALSE,include=TRUE}
meta_TR<-importMetaMatrix("faces/faceDR")
meta_T<-importMetaMatrix("faces/faceDS")
meta_TR<-meta_TR[meta_TR$n %in% rownames(dat),] # Remove any examples from metadata that are not in the data.
meta_T<-meta_T[meta_T$n %in% rownames(dat),]
dat_TR<-dat[meta_TR$n,]
dat_T<-dat[meta_T$n,]
rm(dat)
```

```{r metadata barplot,echo=FALSE,fig.height=6,fig.width=9,fig.align='center'}
foo<-function() {p1<-qplot(sex,data=meta_TR)+ggtitle("Sex")+coord_flip()+theme_classic()
p2<-qplot(age,data=meta_TR)+ggtitle("Age")+coord_flip()+theme_classic()
p3<-qplot(race,data=meta_TR)+ggtitle("Race")+coord_flip()+theme_classic()
p4<-qplot(face,data=meta_TR)+ggtitle("Face")+coord_flip()+theme_classic()
p5<-qplot(prop,data=meta_TR)+ggtitle("Prop")+coord_flip()+theme_classic()
grid.arrange(p1,p2,p3,p4,p5,tableGrob(count(meta_TR[,2:3]),rows=NULL),ncol=3,top="Exploratory Data Analysis")}
foo()
```
## Average faces
```{r cmeans,echo=FALSE,,fig.height=3,fig.width=9}

par(mfrow=c(1,3))
foo<-function() {cmeans = colMeans(dat_TR)
  plotImage(cmeans,"Average Face")
plotImage(colMeans(dat_TR[meta_TR[meta_TR$sex=="female",]$n,]),"Average Female Face")
plotImage(colMeans(dat_TR[meta_TR[meta_TR$sex=="male",]$n,]),"Average Male Face")}
foo()
```
## Eigen faces
```{r pca}
pca.res<-prcomp(dat_TR,center=TRUE,rank. = params$n_features) # formal pca calculation using prcomp()

```

```{r PCA diagnostics, echo=FALSE,fig.height=7,fig.width=10}
foo<-function(){
eigs <- (pca.res$sdev)^2
x<-data.frame(Sdev = sqrt(eigs),
  Proportion = round(eigs/sum(eigs),digits = 3),
  Cumulative = round(cumsum(eigs)/sum(eigs),digits=3))
p1<-fviz_screeplot(pca.res,addlabels=TRUE)
p2<-tableGrob(x[seq(20),c(2,3)])
grid.arrange(p1,p2,ncol=2)}
foo()
```


```{r plot eigen faces,echo=FALSE}
par(mfrow = c(2, 5))
par(oma = rep(2, 4), mar = c(0, 0, 3, 0))
for(i in seq(1:10)) {
  plotImage(pca.res$rotation[,i],paste0(i))
}
```
# Reconstruct Faces
```{r Reconstruct Matrix}
restr <- pca.res$x[,1:(params$n_features)] %*% t(pca.res$rotation[,1:(params$n_features)])
restr <- scale(restr, center = -1 * pca.res$center, scale=FALSE)
```

```{r plot reconstructed faces}
par(mfrow = c( 2,4))
par(mar = c(0, 0, 3, 0
                             ))
for(i in sample(seq(nrow(dat_TR)),8)) {
  plotImage(dat_TR[i,],meta_TR[i,]$n)
  plotImage(restr[i,],paste0(meta_TR[i,]$n,"R"))
}
```

# ML with Caret

Max Kuhn's `caret` package offers a modular framework for the preparation and deployment of numerous ML models. The `caret` system is intuitive and well [documented](https://topepo.github.io/caret/index.html).

I have decided to use near-zero variance filtering to remove pixels with no or very little variance. I.e., pixels that are almost always black. All models deal with uninformative variables in some way--sometimes poorly. Removing them outright during pre-processing avoids that altogether while also reducing the computational burden. As per the `caret` documentation:

>NZV filtering removes features that satisfy either of the following two conditions:
1. the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data.
2. the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases.

I have modified the filtering to require only five percent of values to be unique, which translates into an allowance of nearly `r round(nrow(dat_TR)*.01)` permitted outliers. I imagine these as images which extend farther into the black space than most. The reasoning to do this is that some images feature larger hair or hat, which could be sparse, yet informative latent features.



```{r pre-processing data splitting}
sel_class<-"sex"
nzv<-nearZeroVar(dat_TR, uniqueCut = 1) # one percent of values must be unique
non_nzv<-seq(ncol(dat_TR))[-nzv]
i<-rep(255,128*128)
i[nzv]<-1
plotImage(i,"Pixels Filtered by NZV")

training<-data.frame(Class=meta_TR[[sel_class]],dat_TR[,-nzv])
test<-data.frame(Class=meta_T[[sel_class]],dat_T[,-nzv])
preProcValues <- preProcess(training, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, training)
testTransformed<-predict(preProcValues, test)
```
After pre-processing, the transformed training data generates four models. More information about each model can be found in the `caret` documentation. I chose these models to cover a variety of types: boosted, regression, ensemble, decision tree/rule-based, and regression. This panel is not exhaustive by any means.

```{r model generation,eval=FALSE}
liftCtrl <- trainControl(method = "cv", classProbs = TRUE,
                     summaryFunction = twoClassSummary)
c5 <- train(Class ~ ., data = trainTransformed,
                 method = "C5.0", metric = "ROC",
                 tuneLength = 10,
                 trControl = liftCtrl,
                 control = C50::C5.0Control(earlyStopping = FALSE))
fda<- train(Class ~ ., data = trainTransformed,
                  method = "fda", metric = "ROC",
                  tuneLength = 20,
                  trControl = liftCtrl)
glmBoost_grid = expand.grid(mstop = c(50, 100, 150, 200, 250, 300),
                           prune = c('yes', 'no'))
glmboost<-train(Class~.,data=trainTransformed,
                method="glmboost",metric='ROC',
                trControl=liftCtrl,tuneGrid=glmBoost_grid)
XGB_grid <- expand.grid(nrounds=c(100,200,300,400), 
                         max_depth = c(3:7),
                         eta = c(0.05, 1),
                         gamma = c(0.01),
                         colsample_bytree = c(0.75),
                         subsample = c(0.50),
                         min_child_weight = c(0))
rf_fit <- train(Class ~., data = trainTransformed, method = "xgbTree",
                trControl=liftCtrl,
                tuneGrid = XGB_grid,
                tuneLength = 10,
                metric='ROC')
```

```{r get models,echo=FALSE,eval=TRUE}
# The models have been generated with the generateModels.R script.
load(file.path("models","sexclean_lift_models.Rdata"))
```

The models 

```{r model analysis,message=FALSE}
plotVarImp<-function(model,title=NULL) {
imp<-varImp(model,scale=TRUE)$importance
imp<-cbind(imp,i=as.numeric(stringr::str_remove(rownames(imp),"X")))
imp<-imp[order(imp$i),]
i<-rep(0,128*128)
i[non_nzv]<-imp$Overall
d <- matrix(i, nrow = sqrt(length(i)))
image(
    d[, nrow(d):1],
    col = viridis::viridis(500),
    axes = FALSE,
    main=paste0("varImp ",ifelse(is.null(title),yes = "",no = title)))
}
par(mfrow = c( 1,2))
par(mar = c(0, 0, 3, 0))
{plotVarImp(fda,title="FDA")
plotVarImp(glmboost,title="GLM Boost")
plotVarImp(rf_fit,title="XGBoost Tree")
plotVarImp(c5,title="C5.0")}
```

```{r lift plots}
lift_results <- data.frame(Class = testTransformed$Class)
lift_results$FDA <- predict(fda, testTransformed, type = "prob")[,"female"]
lift_results$XGB <- predict(rf_fit, testTransformed, type = "prob")[,"female"]
lift_results$C5.0 <- predict(c5, testTransformed, type = "prob")[,"female"]
lift_results$GLMB <- predict(glmboost, testTransformed, type = "prob")[,"female"]
lift_obj <- lift(Class ~ FDA + XGB + C5.0+GLMB, data = lift_results)
ggplot(lift_obj, values = 60)
```

```{r confusion mats}
p_results<-data.frame(Class=test$Class)
p_results$FDA <- predict(fda, testTransformed)
p_results$XGB <- predict(rf_fit, testTransformed)
p_results$C5.0 <- predict(c5, testTransformed)
p_results$GLMB <- predict(glmboost, testTransformed)
cMat_FDA<-confusionMatrix(data = p_results$FDA, reference = p_results$Class)
cMat_XGB<-confusionMatrix(data = p_results$XGB, reference = p_results$Class)
cMat_C5.0<-confusionMatrix(data = p_results$C5.0, reference = p_results$Class)
cMat_GLMB<-confusionMatrix(data = p_results$GLMB, reference = p_results$Class)
```

```{r tables,echo=FALSE}
kable(cMat_FDA$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("FDA"= 3))
kable(cMat_XGB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("XGBoost"= 3))
kable(cMat_C5.0$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("C5.0"= 3))
kable(cMat_GLMB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("GLMBoost"= 3))
```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
```{r performance stats}
perf<-rbind(FDA=c(cMat_FDA$overall["Accuracy"],cMat_FDA$byClass[c(1,2,5,6,7,8,9,10,11)]),
                  XBG=c(cMat_XGB$overall["Accuracy"],cMat_XGB$byClass[c(1,2,5,6,7,8,9,10,11)]),
                        C5.0=c(cMat_C5.0$overall["Accuracy"],cMat_C5.0$byClass[c(1,2,5,6,7,8,9,10,11)]),
                              GMLBoost=c(cMat_GLMB$overall["Accuracy"],cMat_GLMB$byClass[c(1,2,5,6,7,8,9,10,11)]))
kable(perf,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
```



# References



























