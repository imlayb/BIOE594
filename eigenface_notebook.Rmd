---
title: "Eigenface Exploration"
author: "Ben Imlay"
params:
  n_features: 1000
bibliography: eigenfaces.bib
output:
  html_document:
    theme: cosmo
  html_notebook:
    theme: cosmo
---
```{r required packages,message=FALSE,warning=FALSE}
source("eigenfaces.R")
library(plyr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(caret)
```

# Introduction

### Face Data

The 1991 article *Eigenfaces for Recognition* was formative for the multimedia research fields [@Turk1991]. The MIT Faces Recognition Project database, which predates the *Eigenfaces for Recognition* article, has since become a benchmark for computer vision and machine learning.


The data consists of 3993 pictures of human faces at a resolution of 128 x 128 in 8-bit grayscale. Data can be obtained from [MIT](http://courses.media.mit.edu/2004fall/mas622j/04.projects/faces/).

### Objective

My objective was two-fold, a data science exploration of dimension reduction in the context of face images and a classification of an image metafeature such as gender using the image itself.

# Methods

## Data Import
The eigenface rawdata is packaged with this repository and can be loaded with the following code. Two pictures were removed from the rawdata as provided from MIT because they were of incorrect dimensions: 2416 and 2412. Moreover, 22 faces are removed for having no parsed information.

```{r data import, message=FALSE, warning=FALSE, include=TRUE}
source("eigenfaces.R")
dat<-importFaceMatrix()
```

Each picture is represented as vector of length $128^{2}=16,384$. This vector can be reshaped into a matrix, which can be colored with a color scale — grayscale in our case. Three random faces have been generated below.

```{r plot images,fig.height=3,fig.width=9,collapse=TRUE,results= 'hold'}
#This code may be run multiple times to see the huge diversity of images in the data.
par(mfrow=c(1,3))
selected<-sample(rownames(dat),3)
{plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])}
```

## Metadata Import and Train/Test Set Splitting

The metadata comes from MIT already split into two sets, training and testing. However, there are compositional biases between the two sets. Therefore, the sets will be combined and split later using a true random split.

```{r meta import,message=FALSE,warning=FALSE,include=TRUE}
meta_TR<-importMetaMatrix("faces/faceDR")
meta_T<-importMetaMatrix("faces/faceDS")
meta_TR<-meta_TR[meta_TR$n %in% rownames(dat),] # Remove any examples from metadata that are not in the data.
meta_T<-meta_T[meta_T$n %in% rownames(dat),]
meta<-rbind(meta_TR,meta_T)
rownames(meta)<-meta$n
meta<-meta[rownames(dat),]
#rm(dat)
```

```{r train test split}
set.seed(1234)
trainIndex <- createDataPartition(meta$sex, p = .8, 
                                  list = FALSE, 
                                  times = 1)
meta_TR<-meta[trainIndex,]
meta_T<-meta[-trainIndex,]
dat_TR<-dat[trainIndex,]
dat_T<-dat[-trainIndex,]
```

The training set consists of `r nrow(dat_TR)` faces, and the test set consists `r nrow(dat_T)` observations. There are a handful of entries in the data that are not contained in the meta. And there are numerous images which do not contain any information. Both have been removed. However, as we will see later, there are dozens of images that feature people turned to the side, dark images, and images with a single non-black pixel.

## Metadata EDA

```{r metadata barplot,echo=FALSE,fig.height=6,fig.width=9,fig.align='center'}
foo<-function() {p1<-qplot(sex,data=meta_TR)+ggtitle("Sex")+coord_flip()+theme_minimal()
p2<-qplot(age,data=meta_TR)+ggtitle("Age")+coord_flip()+theme_minimal()
p3<-qplot(race,data=meta_TR)+ggtitle("Race")+coord_flip()+theme_minimal()
p4<-qplot(face,data=meta_TR)+ggtitle("Face")+coord_flip()+theme_minimal()
p5<-qplot(prop,data=meta_TR)+ggtitle("Prop")+coord_flip()+theme_minimal()
grid.arrange(p1,p2,p3,p4,p5,tableGrob(table(as.data.frame(meta_TR[,2:3]))),ncol=3,top="Exploratory Data Analysis")}
foo()
```

### EDA Takeaways

Each of the available metadata fields are highly biased. Most pictures are of white, adult, male subjects. The two largest groups are adult females and adult males. I considered creating a subset of only white adults, but I chose against it for the sake of generalizability. Allowing small sub-groups into the training and test sets will potentially hamper performance, but I am more interested in the relative performance of different models than in absolute performance.

## Average faces

The average face is computed by taking the average of each column across all of the training data. This vector is important for centering each pixel for subsequent decomposition. The average faces reveal a few trends. Namely that the female face has less prominent ears; presumably because more female faces have long hair. Moreover, the eyes of the male face appear slightly less tilted, and the face appears slightly larger.

```{r average face calc}
c_means = colMeans(dat_TR)
c_dat_TR<-scale(dat_TR, center = 1 * c_means, scale=FALSE)
```

```{r cmeans,echo=FALSE,,fig.height=3,fig.width=9}
par(mfrow=c(1,3))
foo<-function() {
  plotImage(c_means,"Average Face")
plotImage(colMeans(dat_TR[meta_TR[meta_TR$sex=="female",]$n,]),"Average Female Face")
plotImage(colMeans(dat_TR[meta_TR[meta_TR$sex=="male",]$n,]),"Average Male Face")}
foo()
```

## SVD and Eigenfaces

```{r load svd,echo=FALSE}
# SVD calculation takes a long time
load(file.path("models","svd.Rdata"))
```

```{r SVD,eval=FALSE}
### REQUIRED ON FIRST RUN
c_means<-colMeans(dat_TR)
c_dat_TR<-scale(dat_TR, center = 1 * c_means, scale=FALSE) # Centers each pixel.
svd.res<-svd(c_dat_TR)
save(list = c("svd.res"),file = file.path("models","svd.Rdata"))
```


SVD is a decomposition method that is applicable for any matrix, and is defined by the following equation:

$$A=USV^T$$

In R, `svd()` produces `u`, which represents $U$; `d`. which is a vector for $S$; `v`, which represents $V^T$. SVD is simply a generalization of the eigendecomposition. For our rectangular matrix, the eigen values correspond to the same eigen values one would find through a the traditional $A^{T}Au_{i}=u_{i}\lambda_{i}$ route. I chose SVD to practice working with the results of that decomposition method, and the computational times were similar for either method.

## SVD Dimension Reduction Results

The following graphics exhibit that the first few eigenvalues capture a sizable chunk of the total variance - 50% by the third eigenvalue. The proportion of variance explained falls to less than 1% by the 13th eigenvalue. This supported my choice to use only 100 principal components for classification later on.

```{r SVD diagnostics,fig.height=7,fig.width=10,echo=FALSE}
foo2<-function(d) {
  x<-data.frame(ith_Eig=1:d,
                Proportion = round(svd.res$d^2/sum(svd.res$d^2),3)[1:d],
                Cumulative = round(cumsum(svd.res$d^2)/sum(svd.res$d^2),3)[1:d])
  return(x)
}
x<-foo2(100)
p1<-ggplot(data=x[1:5,])+
  geom_line(aes(x=ith_Eig,y=Proportion),linetype='dashed')+
  geom_point(aes(x=ith_Eig,y=Proportion))+
  geom_label(aes(x=ith_Eig,y=Proportion,label=Proportion),nudge_x = .5,nudge_y = .005)+
  theme_classic()+ggtitle("Proportion of Variance Explained")
p2<-tableGrob(x[1:20,c(2,3)])
grid.arrange(p1,p2,ncol=2)
```

Eigenfaces are created from $V_{d,k}$ where $k$ is the number of desired eigenfaces. In R, this is simply `t(svd.res$v[,1:k])`, where each columns is an eigenvector. These eigenfaces appear to capture key differences between people's faces. The first appears to capture a contrast between the background and the face. The second appears to capture a contrast between the background and hair. The third appears to capture some contrast involving the forehead. 5 and 6 appear to capture different lighting angles.

Many of these eigenfaces convey conditions of the picture rather than information about that attributes of the shape. Nonetheless, eigenfaces such as 9 may be associated with gender.

```{r plot eigen faces,echo=FALSE,fig.height=6,fig.width=9}
par(mfrow = c(2, 5))
par(mar = c(0, 0, 4.1, 0))
for(i in seq(1:10)) {
  plotImage(t(svd.res$v[,i]),paste0(i))
}
```

## Reconstruct Faces

SVD can be used to make low-rank approximations of points within the original matrix. In our case, we can reconstruct the original matrix using `r params$n_features` features, which is specified as a parameter in this document.

```{r Reconstruct Matrix}
S<-diag(svd.res$d) # Creating diagonal matrix from eigenvalue vector
Ss<-S[,1:params$n_features] # Taking n column vectors of S
V<-svd.res$v[,1:params$n_features] # Taking n column vectors of V 
restr<-svd.res$u %*% Ss %*% t(V) # U * S * V^T
restr <- scale(restr, center = -1 * c_means, scale=FALSE)
```


These reconstructed faces can be plotted and compared to the original images. By `r params$n_features`, the approximation is excellent. Only regions of very high, random variance such as the areas around the edges of the head are still blurry. At this point, the images could pass as poorly compressed JPEG images!

```{r plot reconstructed faces,echo=FALSE}
par(mfrow = c( 2,4))
par(mar = c(0, 0, 3, 0
                             ))
for(i in sample(seq(nrow(dat_TR)),8)) {
  plotImage(dat_TR[i,],meta_TR[i,]$n)
  plotImage(restr[i,],paste0(meta_TR[i,]$n,"R"))
}
rm(restr)
```


Moreover, we can see how adding additional bases for a reconstruction iteratively improves the image quality.

```{r Reconstruct slow,eval=FALSE}
sel<-sample(1:nrow(dat_TR),size = 3)
for (nul in seq(1,70)) {
  if (nul < 10) {name = paste('000',nul,'plot.jpg',sep='')}
  if (nul < 100 && nul >= 10) {name = paste('00',nul,'plot.jpg', sep='')}
  if (nul >= 100) {name = paste('0', nul,'plot.jpg', sep='')}
i<-nul*1
Sss<-S[,1:i]
V<-svd.res$v[,1:i]
gx<-svd.res$u[sel,] %*% Sss %*% t(V) # U * S * V^T
gx <- scale(gx, center = -1 * c_means, scale=FALSE)
png(file.path("images","slow",name))
par(mar = c(0,0,4.1,0))
plotImage(gx[1,],title = i)
dev.off()
}
sel<-sample(1:nrow(dat_TR),size = 3)
for (nul in seq(1,60)) {
  if (nul < 10) {name = paste('000',nul,'plot.jpg',sep='')}
  if (nul < 100 && nul >= 10) {name = paste('00',nul,'plot.jpg', sep='')}
  if (nul >= 100) {name = paste('0', nul,'plot.jpg', sep='')}
i<-nul*15
Sss<-S[,1:i]
V<-svd.res$v[,1:i]
gx<-svd.res$u[sel,] %*% Sss %*% t(V) # U * S * V^T
gx <- scale(gx, center = -1 * c_means, scale=FALSE)
png(file.path("images","fast",name))
par(mar = c(0,0,4.1,0))
plotImage(gx[1,],title = i)
dev.off()
}
system("convert -delay 15 -loop 0 images/slow/*.jpg images/slow/c.gif")
system("convert -delay 15 -loop 0 images/fast/*.jpg images/fast/c.gif")
```

![iterative improvement](images/slow/c.gif)
![fast iterative improvement](images/fast/c.gif)

# ML with Caret

Max Kuhn's `caret` package offers a modular framework for the preparation and deployment of numerous ML models. The `caret` system is intuitive and well [documented](https://topepo.github.io/caret/index.html).

## NZV pre-processing

I have decided to use near-zero variance filtering to remove pixels with no or very little variance. I.e., pixels that are almost always black. All models deal with uninformative variables in some way--sometimes poorly. Removing them outright during pre-processing avoids that altogether while also reducing the computational burden. As per the `caret` documentation:

>NZV filtering removes features that satisfy either of the following two conditions:
1. the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data.
2. the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases.

I have modified the filtering to require only five percent of values to be unique, which translates into an allowance of nearly `r round(nrow(dat_TR)*.01)` permitted outliers. I imagine these as images which extend farther into the black space than most. The reasoning to do this is that some images feature larger hair or hat, which could be sparse, yet informative latent features.

```{r pre-processing data splitting,fig.height=6,fig.width=7}
sel_class<-"sex"
nzv<-nearZeroVar(dat_TR, uniqueCut = 5) # five percent of values must be unique
non_nzv<-seq(ncol(dat_TR))[-nzv]
i<-rep(255,128*128)
i[nzv]<-1
plotImage(i,"Pixels Filtered by NZV")

training<-data.frame(Class=meta_TR[[sel_class]],dat_TR[,-nzv])
test<-data.frame(Class=meta_T[[sel_class]],dat_T[,-nzv])
preProcValues <- preProcess(training, method = c("center"))
trainTransformed <- predict(preProcValues, training)
testTransformed<-predict(preProcValues, test)
```

After pre-processing, the transformed training data generates four models. More information about each model can be found in the `caret` documentation. I chose these models to cover a variety of types: boosted, regression, ensemble, decision tree/rule-based, and regression. This panel is not exhaustive by any means. The NZV pre-processing resulted in a total of `r length(non_nzv)` features. This proved to be extremely computationally expensive; training the models took nearly 32 hours. 

```{r model generation,eval=FALSE}
liftCtrl <- trainControl(method = "cv", classProbs = TRUE,
                     summaryFunction = twoClassSummary)
c5 <- train(Class ~ ., data = trainTransformed,
                 method = "C5.0", metric = "ROC",
                 tuneLength = 10,
                 trControl = liftCtrl,
                 control = C50::C5.0Control(earlyStopping = FALSE))
fda<- train(Class ~ ., data = trainTransformed,
                  method = "fda", metric = "ROC",
                  tuneLength = 20,
                  trControl = liftCtrl)
glmBoost_grid = expand.grid(mstop = c(50, 100, 150, 200, 250, 300),
                           prune = c('yes', 'no'))
glmboost<-train(Class~.,data=trainTransformed,
                method="glmboost",metric='ROC',
                trControl=liftCtrl,tuneGrid=glmBoost_grid)
XGB_grid <- expand.grid(nrounds=c(100,200,300,400), 
                         max_depth = c(3:7),
                         eta = c(0.05, 1),
                         gamma = c(0.01),
                         colsample_bytree = c(0.75),
                         subsample = c(0.50),
                         min_child_weight = c(0))
rf_fit <- train(Class ~., data = trainTransformed, method = "xgbTree",
                trControl=liftCtrl,
                tuneGrid = XGB_grid,
                tuneLength = 10,
                metric='ROC')
```

```{r get models,echo=FALSE,eval=TRUE}
# The models have been generated with the generateModels.R script.
load(file.path("models","sexliftcent_models.Rdata"))
```

### NZV-based Variable Importance Figures

The models are then analyzed through `varImp` in `caret`. The plots show both how different classifiers use different numbers of features and how the models *see* the data. Intuitively, some of the models picked up on important variance around the eyes and the peripheries of the head. There is also evidence of differences in how the different models deal with high dimensionality; C5.0 appears to be highly inclusive whereas FDA identifies only a handful of pixels for classification.

```{r model analysis,message=FALSE,echo=FALSE}
plotVarImp<-function(model,title=NULL) {
imp<-varImp(model,scale=TRUE)$importance
imp<-cbind(imp,i=as.numeric(stringr::str_remove(rownames(imp),"X")))
imp<-imp[order(imp$i),]
i<-rep(0,128*128)
i[non_nzv]<-imp$Overall
d <- matrix(i, nrow = sqrt(length(i)))
image(
    d[, nrow(d):1],
    col = viridis::viridis(500),
    axes = FALSE,
    main=paste0("varImp ",ifelse(is.null(title),yes = "",no = title)))
}
par(mfrow = c( 1,2))
par(mar = c(0, 0, 3, 0))
{plotVarImp(fda,title="FDA")
plotVarImp(glmboost,title="GLM Boost")
plotVarImp(rf_fit,title="XGBoost Tree")
plotVarImp(c5,title="C5.0")}
```

### NZV-based Cumulative Gains Plot

The cumulative gains plot is typically used for response vs. control situations where one is interested in capturing some subset of the most likely responders. The X-axis represents a descending list of high-confidence female predictions for each model. For example, the first 10% on the X-axis is the 10% most confident female predictions for each model. The Y-axis is simply the percentage of responders (females) found. If XGB passes through X=10% and Y=8%, that means it found 8% of the total females in the 10% of its highest confidence predictions. This plot can give insight into model performance even if the accuracies are ultimately similar between models. This metric is a bit more robust to extremely difficult test set images. This is reasonable for us because some of the images are quite ambiguous.

The results for the NZV pre-processing alone show a definitive lead for the XGB and C5.0 modells. This makes sense as they are most robust to a high number of features. Linear models are penalized heavily for too many coefficients. And we know that that important pixels may be different from image to image. 

```{r lift results and plots,echo=F}
lift_results <- data.frame(Class = testTransformed$Class)
lift_results$FDA <- predict(fda, testTransformed, type = "prob")[,"female"]
lift_results$XGB <- predict(rf_fit, testTransformed, type = "prob")[,"female"]
lift_results$C5.0 <- predict(c5, testTransformed, type = "prob")[,"female"]
lift_results$GLMB <- predict(glmboost, testTransformed, type = "prob")[,"female"]
lift_obj <- lift(Class ~ FDA + XGB + C5.0+GLMB, data = lift_results)
ggplot(lift_obj, values = 60)
```

### NZV-based Results

The actual performances of the models were rather good. The performance using a true random test set and training set was much better than using the MIT supplied sets. The accuracies milled around the 80% mark, which is pretty good considering how many difficult images are lurking in the data set. When using pixels as features, tree-based models had an obvious advantage; they can weight many weak classifiers to yield strong classifiers. The linear model and discriminant analysis technique search for a small, robust set of features, which would fail if an image is even slightly off center. This intuitive advantage is seen in the higher accuracy for the tree-based models.

####Test Set Confusion Matrices

```{r confusion mats,echo=FALSE}
p_results<-data.frame(Class=test$Class)
p_results$FDA <- predict(fda, testTransformed)
p_results$XGB <- predict(rf_fit, testTransformed)
p_results$C5.0 <- predict(c5, testTransformed)
p_results$GLMB <- predict(glmboost, testTransformed)
cMat_FDA<-confusionMatrix(data = p_results$FDA, reference = p_results$Class)
cMat_XGB<-confusionMatrix(data = p_results$XGB, reference = p_results$Class)
cMat_C5.0<-confusionMatrix(data = p_results$C5.0, reference = p_results$Class)
cMat_GLMB<-confusionMatrix(data = p_results$GLMB, reference = p_results$Class)
```

```{r tables,echo=FALSE}
kable(cMat_FDA$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("FDA"= 3))
kable(cMat_XGB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("XGBoost"= 3))
kable(cMat_C5.0$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("C5.0"= 3))
kable(cMat_GLMB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("GLMBoost"= 3))
```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

#### Accuracy Metrics

```{r performance stats, echo=FALSE}
perf<-rbind(FDA=c(cMat_FDA$overall["Accuracy"],cMat_FDA$byClass[c(1,2,5,6,7,8,9,10,11)]),
                  XGB=c(cMat_XGB$overall["Accuracy"],cMat_XGB$byClass[c(1,2,5,6,7,8,9,10,11)]),
                        C5.0=c(cMat_C5.0$overall["Accuracy"],cMat_C5.0$byClass[c(1,2,5,6,7,8,9,10,11)]),
                              GMLBoost=c(cMat_GLMB$overall["Accuracy"],cMat_GLMB$byClass[c(1,2,5,6,7,8,9,10,11)]))
kable(perf,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
```

## Principal Component Pre-processing

To see how far I could push the accuracy and how the different models might respond to a different pre-processing scheme, my second approach was to use each image's projections in the first 100 principal compenents as features. Though I still remove NZV pixels to enable scaling of each pixel feature. As we have seen in the SVD plots, the eigenfaces appear to sometimes catch gender associated traits such as shading around where long hair might be. This encouraged me that using the eigenfaces may yield better performance in some way over pixels alone.

Moreover, the principal component scheme trained much, much faster than the NZV pre-processing alone. The models finished training in approximately 16 hours.

```{r PCA, eval=F}
### REQUIRED ON FIRST RUN
pca.res<-prcomp(dat_TR[,-nzv],center=T,scale. = T,rank. = 100)
save(list = c("pca.res"),file = file.path("models","prcomp.Rdata"))
```

```{r load prcomp,echo=F}
load(file.path("models","prcomp.Rdata"))
pca.ind <- pca.res$x
```

### PCA Pre-processing

```{r PCA pre-processing}
##
sel_class<-"sex"
training<-data.frame(Class=meta_TR[[sel_class]],pca.res$x) ### Only thing needed to change selected class

##
preProcValues <- preProcess(training, method = c("center","scale"))
trainTransformed <- predict(preProcValues, training)
test_PCA<- predict(pca.res, newdata = dat_T[,-nzv])
test_PCA<-data.frame(Class=meta_T[[sel_class]],test_PCA)
testTransformed<-predict(preProcValues, test_PCA)
```

The script used for generating the models is nearly identical to the script used for the NZV pre-processed run.

```{r get PCA models,echo=FALSE,eval=TRUE}
# The models have been generated with the generateModels.R script.
load(file.path("models","sex_PCA_liftcent_models.Rdata"))
```

### PCA Variable Importance

The models only had 100 features to train with, but the feature importance metrics tell the same story as with NZV pre-processing alone. FDA chose relatively few features, followed by GLMB, XGB, and C5.0. However, of interest here is how most of the models weighted certain principal components relatively higher than others - namely PC9. The ninth eigenface appears to show the shading of a mustache, which would be a helpful attribute in classifying gender.

```{r PCA varimp,echo=F}
imp<-varImp(fda,scale=T)$importance
imp<-cbind(imp,varImp(rf_fit,scale=T)$importance)
imp<-cbind(imp,varImp(c5,scale=T)$importance)
imp<-cbind(imp,varImp(glmboost,scale=T)$importance)
imp$PC<-rownames(imp)
imp<-imp[gtools::mixedsort(imp$PC),]
names(imp)<-c("FDA","XGB","C5.0","GLMB","Component")
n<-100
kable(imp[1:n,c(5,1,2,3,4)],"html",row.names = FALSE) %>% # need a better way to do this
  kable_styling(bootstrap_options = "striped", full_width = T) %>%
      scroll_box(width = "100%", height = "200px")
plotImage(t(svd.res$v[,9]),paste0(9))
```


### PCA Cumulative Gains Plot

The cumulative gains plot tells about the same story as for NZV pre-processing alone. Tree-based classifiers may have an overall edge in scenarios such as image processing where classifications can be the result of many, ambiguous features.

```{r PCA lift results and plots,echo=F}
lift_results <- data.frame(Class = testTransformed$Class)
lift_results$FDA <- predict(fda, testTransformed, type = "prob")[,"female"]
lift_results$XGB <- predict(rf_fit, testTransformed, type = "prob")[,"female"]
lift_results$C5.0 <- predict(c5, testTransformed, type = "prob")[,"female"]
lift_results$GLMB <- predict(glmboost, testTransformed, type = "prob")[,"female"]
lift_obj <- lift(Class ~ FDA + XGB + C5.0+GLMB, data = lift_results)
ggplot(lift_obj, values = 60)+ggtitle("")
```

### PCA Results

The accuracies were marginally worse overall. This came to my great surprise. My reasoning is that despite the huge number of features when using NZV pre-processing alone the models are highly effective at weighting each pixel - even feature-limited models such as FDA and GLMB. Moreover, the margins of loss are quite small. In reality, the PCA models performed about the same while taking much less time to train.

#### Confusion Mats

```{r PCA confusion mats,echo=FALSE}
p_results<-data.frame(Class=testTransformed$Class)
p_results$FDA <- predict(fda, testTransformed)
p_results$XGB <- predict(rf_fit, testTransformed)
p_results$C5.0 <- predict(c5, testTransformed)
p_results$GLMB <- predict(glmboost, testTransformed)
cMat_FDA<-confusionMatrix(data = p_results$FDA, reference = p_results$Class)
cMat_XGB<-confusionMatrix(data = p_results$XGB, reference = p_results$Class)
cMat_C5.0<-confusionMatrix(data = p_results$C5.0, reference = p_results$Class)
cMat_GLMB<-confusionMatrix(data = p_results$GLMB, reference = p_results$Class)
```

```{r PCA tables,echo=FALSE}
kable(cMat_FDA$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("FDA"= 3))
kable(cMat_XGB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("XGBoost"= 3))
kable(cMat_C5.0$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("C5.0"= 3))
kable(cMat_GLMB$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("GLMBoost"= 3))
```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

### PCA Accuracy Metrics Table

```{r PCA performance stats, echo=FALSE}
perf<-rbind(FDA=c(cMat_FDA$overall["Accuracy"],cMat_FDA$byClass[c(1,2,5,6,7,8,9,10,11)]),
                  XGB=c(cMat_XGB$overall["Accuracy"],cMat_XGB$byClass[c(1,2,5,6,7,8,9,10,11)]),
                        C5.0=c(cMat_C5.0$overall["Accuracy"],cMat_C5.0$byClass[c(1,2,5,6,7,8,9,10,11)]),
                              GMLBoost=c(cMat_GLMB$overall["Accuracy"],cMat_GLMB$byClass[c(1,2,5,6,7,8,9,10,11)]))
kable(perf,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
```

## Microsoft Azure Face Detection

Microsoft's cloud computing platform, Azure, offers cognitive services inlcuding facial recognition. With models of my own performing reasonably well, I wanted to compare my results with cutting-edge facial recognition services. The Azure results will give insight into the inherent problems within the dataset. If their model can't perform perfectly, then it is unlikely that any contemporary model would perform better. There are indeed problematic pictures and ambiguous individuals which could be skewing accuracy of my models.

As such, I processed the entire MIT face recognition dataset through the cognitive services API from Azure. Trial accounts can processed up to 20 images per minute. I accomplished this with two scripts. The first script generates JPEG images from all of the binary files, sends the jpeg to the API, and then collects the result. The second script binds those results into a TSV and also writes a list of images for which the API does not return a result.

There are a variety of parameters for each API request. More information can be found [here](https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236).

The parameters decide which model to use and which features to request back. I used the following:

>`"?returnFaceId=true&returnFaceLandmarks=true&returnFaceAttributes=age,gender,smile,facialHair,glasses,emotion,accessories&recognitionModel=recognition_02"`

This means I requested the following features:

1. Face landmarks
2. Age
3. Smile
4. Facial hair
5. Glasses
6. Emotion
7. Accessories
8. Use of recongition model 2, released in March 2019.


```{r cognitive services api, eval=F}
# Processing your own images requires a key file called 'keys' in json format with an entry called 'key1'
source("Azure_classification.R")
source("Azure_processing.R")
```

```{r loading Azure results}
azure_df<-data.table::fread("Azure_Results.tsv",sep = "\t",header = T,data.table = F)
azure_df$n<-as.character(azure_df$n);azure_df$faceAttributes.gender<-as.factor(azure_df$faceAttributes.gender)
rownames(azure_df)<-azure_df$n
null_images<-data.table::fread("Failed_Azure_Images.tsv",header=T,data.table=F)
meta_azure<-meta[azure_df$n,] #only include images that Azure successfully classified
meta_azure$sex<-as.factor(meta_azure$sex)
```

### Reannotated EDA

The Azure Congitive Services API did remarkably well based on the ratio of female to males. The fact that the API could preserve the imbalanced gender ratio is a good sign as each classification was entirely independent of the other images.

```{r Azure EDA,echo=F,fig.height=6,fig.width=6,results='hide',message=F}
#tableGrob(table(as.data.frame(meta_TR[,2:3])))
foo<-function() {p1<-qplot(faceAttributes.gender,data=azure_df)+ggtitle("Azure Gender")+coord_flip()+theme_minimal()
p2<-qplot(faceAttributes.age,data=azure_df)+ggtitle("Azure Age")+theme_minimal()
p3<-qplot(sex,data=meta_azure)+ggtitle("Original Gender")+coord_flip()+theme_minimal()
p4<-qplot(age,data=meta_azure)+ggtitle("Original Age")+coord_flip()+theme_minimal()
grid.arrange(p1,p2,p3,p4,ncol=2,top="Azure vs. Original Annotation: Gender & Age")}
foo()
```

### Confusion Matrix - Gender

Below are confusion matrices and performance metrics for all models. The accuracy is only barely higher than the XGB and C5.0 models. I found this result to be spectacular news for both my efforts and the cognitive services API. Though my models are not robust to color images, different size images, non-centered images, and images with objects besides faces, their parity with the state-of-the-art model from Microsoft is reassuring. Simply passing an image directly into an XGB classifier is on par with an enterprise classifier for gender prediction!

```{r Azure tables gender,echo=FALSE}
# hard coded for gender
cMat_Azure<-confusionMatrix(data = azure_df$faceAttributes.gender, reference = meta_azure$sex)
kable(cMat_Azure$table,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F,position="float_left") %>% 
    add_header_above(c("Pred.","Ref."= 2)) %>%
      add_header_above(c("Azure"= 3))
perf<-rbind(FDA=c(cMat_FDA$overall["Accuracy"],cMat_FDA$byClass[c(1,2,5,6,7,8,9,10,11)]),
                  XGB=c(cMat_XGB$overall["Accuracy"],cMat_XGB$byClass[c(1,2,5,6,7,8,9,10,11)]),
                        C5.0=c(cMat_C5.0$overall["Accuracy"],cMat_C5.0$byClass[c(1,2,5,6,7,8,9,10,11)]),
                              GMLBoost=c(cMat_GLMB$overall["Accuracy"],cMat_GLMB$byClass[c(1,2,5,6,7,8,9,10,11)]),
                                  Azure=c(cMat_Azure$overall["Accuracy"],cMat_Azure$byClass[c(1,2,5,6,7,8,9,10,11)]))
kable(perf,"html") %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
```

### Azure Classification Failure

However, the higher performance of the Azure cognitive services was due in part to the fact that it refused to classify some images.

There were `r nrow(null_images)` images of the total `r nrow(dat)` images that failed to be classified by Azure. If it had been forced to predict the gender of those images at random, this would have lowered the overall performance to just below 90%. Below are a sampling of these images. In general, Azure failed on non-direct faces, of which there were many. It also failed on blank images, which is reasonable. But it also failed on various clear images. My guess is that there simply was not enough contrast or resolution to identify facial landmarks. People with glare on their glasses are common among the Azure failures.

```{r Azure fail plot images,fig.height=9,fig.width=9,collapse=TRUE,results= 'hold',message=F,echo=F}
#This code may be run multiple times to see the huge diversity of images in the data.
par(mfrow=c(3,3),mar=c(0,0,4.1,0))
selected<-sample(as.character(null_images$null_images),9)
{plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])
plotImage(dat[selected[4],],selected[4])
plotImage(dat[selected[5],],selected[5])
plotImage(dat[selected[6],],selected[6])
plotImage(dat[selected[7],],selected[7])
plotImage(dat[selected[8],],selected[8])
plotImage(dat[selected[9],],selected[9])}
```

### Azure emotion classification

Though the Azure classifications do not directly line up with provided metadata from MIT, we can plot a few of the high confidence images for each emotion to see what kind of traits the Azure model may be looking for.

#### Surprise

```{r Surprise plots,echo=F,fig.height=3,fig.width=6,results='hide',message=F}
par(mfrow=c(1,3),mar=c(0,0,4.1,0))
selected<-rownames(azure_df[order(-azure_df$faceAttributes.emotion.surprise),])
{
plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])
}
```

#### Anger

```{r Anger plots,echo=F,fig.height=3,fig.width=6,results='hide',message=F}
par(mfrow=c(1,3),mar=c(0,0,4.1,0))
selected<-rownames(azure_df[order(-azure_df$faceAttributes.emotion.anger),])
{
plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])
}
```

#### Sadness

```{r Sadness plots,echo=F,fig.height=3,fig.width=6,results='hide',message=F}
par(mfrow=c(1,3),mar=c(0,0,4.1,0))
selected<-rownames(azure_df[order(-azure_df$faceAttributes.emotion.sadness),])
{
plotImage(dat[selected[1],],selected[1])
plotImage(dat[selected[2],],selected[2])
plotImage(dat[selected[3],],selected[3])
}
```

The Cognitive Services API did remarkably well flagging these images with the respective emotion. However, Azure encodes its predictions as probabilities for each emotional state. This means a threshold would need to be set to extract a single emotional state from an image.

## Conclusions and Future Directions

Through the variety of pre-processing methods, classifiers, and the Azure API, I believe that I have achieved the objectives of this project. I observed the benefits and drawbacks associated with different pre-processing and models. The greatest benefit in this dataset with using eigenfaces instead of images directly was the computational cost. Moreover, I demonstrated that a simple XGB classifier can come within striking distance of enterprise cognitive services in a controlled setting. Given how well Azure did, I am interested in experimentting with mixed models and deep learning. Methods that are better able to interpret many weak, vague signals at once seem to dominate in image processing. 

# References



























